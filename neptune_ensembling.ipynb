{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def multi_roc_auc_score(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    columns = y_true.shape[1]\n",
    "    column_losses = []\n",
    "    for i in range(0, columns):\n",
    "        column_losses.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    return np.array(column_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = ['toxic', 'severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "SINGLE_DIR = '/public/toxic_comments/single_model_predictions'\n",
    "\n",
    "BWL_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'bad_word_logreg.csv')\n",
    "BWCL_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'logreg_bad_word_count.csv')\n",
    "CVDCNN_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'char_vdcnn.csv')\n",
    "CLOGREG_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'logreg_count.csv')\n",
    "GDPCNN_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'glove_dpcnn.csv')\n",
    "GLSTM_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'glove_lstm.csv')\n",
    "GSCNN_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'glove_scnn.csv')\n",
    "TFIDIF_LOGREG_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'logreg_tfidf.csv')\n",
    "WLSTM_VALID_PATH = os.path.join(SINGLE_DIR, 'valid', 'word_lstm.csv')\n",
    "\n",
    "VALID_LABELS_PATH = os.path.join(SINGLE_DIR, 'valid', 'valid_split.csv')\n",
    "\n",
    "BWL_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'bad_word_logreg.csv') \n",
    "BWCL_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'logreg_bad_word_count.csv')\n",
    "CVDCNN_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'char_vdcnn.csv')\n",
    "CLOGREG_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'logreg_count.csv')\n",
    "GDPCNN_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'glove_dpcnn.csv')\n",
    "GLSTM_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'glove_lstm.csv')\n",
    "GSCNN_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'glove_scnn.csv')\n",
    "TFIDIF_LOGREG_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'logreg_tfidf.csv')\n",
    "WLSTM_TEST_PATH = os.path.join(SINGLE_DIR, 'test', 'word_lstm.csv')\n",
    "\n",
    "SAMPLE_SUBMISSION_PATH = os.path.join(SINGLE_DIR, 'test', 'sample_submission.csv')\n",
    "ENSEMBLE_SUBMISSION_PATH = '/output/submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set Level 1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_word_logreg_valid = pd.read_csv(BWL_VALID_PATH)\n",
    "bad_word_count_logreg_valid = pd.read_csv(BWCL_VALID_PATH)\n",
    "char_vdcnn_valid = pd.read_csv(CVDCNN_VALID_PATH)\n",
    "count_logreg_valid = pd.read_csv(CLOGREG_VALID_PATH)\n",
    "glove_dpcnn_valid = pd.read_csv(GDPCNN_VALID_PATH)\n",
    "glove_lstm_valid = pd.read_csv(GLSTM_VALID_PATH)\n",
    "glove_scnn_valid = pd.read_csv(GSCNN_VALID_PATH)\n",
    "tfidf_logreg_valid = pd.read_csv(TFIDIF_LOGREG_VALID_PATH)\n",
    "word_lstm_valid = pd.read_csv(WLSTM_VALID_PATH)\n",
    "\n",
    "labels_valid = pd.read_csv(VALID_LABELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.hstack([bad_word_logreg_valid.drop('id',axis=1), \n",
    "                     bad_word_count_logreg_valid.drop('id',axis=1), \n",
    "                     char_vdcnn_valid.drop('id',axis=1), \n",
    "                     count_logreg_valid.drop('id',axis=1), \n",
    "                     glove_dpcnn_valid.drop('id',axis=1), \n",
    "                     glove_lstm_valid.drop('id',axis=1), \n",
    "                     glove_scnn_valid.drop('id',axis=1), \n",
    "                     tfidf_logreg_valid.drop('id',axis=1), \n",
    "                     word_lstm_valid.drop('id',axis=1)])\n",
    "\n",
    "y_valid_multilabel = labels_valid[LABEL_COLUMNS].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Level 1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_word_logreg_test = pd.read_csv(BWL_TEST_PATH)\n",
    "bad_word_count_logreg_test = pd.read_csv(BWCL_TEST_PATH)\n",
    "char_vdcnn_test = pd.read_csv(CVDCNN_TEST_PATH)\n",
    "count_logreg_test = pd.read_csv(CLOGREG_TEST_PATH)\n",
    "glove_dpcnn_test = pd.read_csv(GDPCNN_TEST_PATH)\n",
    "glove_lstm_test = pd.read_csv(GLSTM_TEST_PATH)\n",
    "glove_scnn_test = pd.read_csv(GSCNN_TEST_PATH)\n",
    "tfidf_logreg_test = pd.read_csv(TFIDIF_LOGREG_TEST_PATH)\n",
    "word_lstm_test = pd.read_csv(WLSTM_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.hstack([bad_word_logreg_test.drop('id',axis=1), \n",
    "                    bad_word_count_logreg_test.drop('id',axis=1), \n",
    "                    char_vdcnn_test.drop('id',axis=1), \n",
    "                    count_logreg_test.drop('id',axis=1), \n",
    "                    glove_dpcnn_test.drop('id',axis=1), \n",
    "                    glove_lstm_test.drop('id',axis=1), \n",
    "                    glove_scnn_test.drop('id',axis=1), \n",
    "                    tfidf_logreg_test.drop('id',axis=1), \n",
    "                    word_lstm_test.drop('id',axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "def fit_cv(X,y,n_splits=10):\n",
    "    estimators,scores = [],[]\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    for train, valid in kf.split(X):\n",
    "        X_train_ = X[train]\n",
    "        y_train_ = y[train]\n",
    "        X_valid_ = X[valid]\n",
    "        y_valid_ =  y[valid]\n",
    "        \n",
    "        estimators_fold = []\n",
    "        for i in tqdm(range(6)):\n",
    "            y_train_one_label = y_train_[:,i]\n",
    "            estimator = CatBoostClassifier(iterations=500, \n",
    "                                           learning_rate=0.02, \n",
    "                                           depth=2, \n",
    "                                           verbose=False)\n",
    "            estimator.fit(X_train_, y_train_one_label)\n",
    "            estimators_fold.append(estimator)\n",
    "        estimators.append(estimators_fold)\n",
    "        \n",
    "        y_valid_pred = []\n",
    "        for estimator in estimators_fold:\n",
    "            y_valid_pred_one_label = estimator.predict_proba(X_valid_)\n",
    "            y_valid_pred.append(y_valid_pred_one_label)\n",
    "        y_valid_pred = np.stack(y_valid_pred, axis=1)[...,1]\n",
    "        score = multi_roc_auc_score(y_valid_, y_valid_pred)\n",
    "        scores.append(score)\n",
    "    return scores, estimators    \n",
    "\n",
    "scores, estimators = fit_cv(X_valid, y_valid_multilabel)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('score average {}\\nscore std {}'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bagged =[]\n",
    "for estimators_fold in estimators:\n",
    "    y_test_pred = []\n",
    "    for estimator in estimators_fold:\n",
    "        y_test_pred_one_label = estimator.predict_proba(X_test)\n",
    "        y_test_pred.append(y_test_pred_one_label)\n",
    "    y_test_pred = np.stack(y_test_pred, axis=1)[...,1]\n",
    "    y_bagged.append(y_test_pred)\n",
    "y_bagged = np.mean(np.stack(y_bagged),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "submission[LABEL_COLUMNS] = y_bagged # this gets 0.9849 on LB\n",
    "submission.to_csv(ENSEMBLE_SUBMISSION_PATH, index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_py3",
   "language": "python",
   "name": "dl_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
