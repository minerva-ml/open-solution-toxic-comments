{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def multi_roc_auc_score(y_true, y_pred):\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    columns = y_true.shape[1]\n",
    "    column_losses = []\n",
    "    for i in range(0, columns):\n",
    "        column_losses.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    return np.array(column_losses).mean()\n",
    "\n",
    "def read_predictions(prediction_dir, concat_mode='concat', per_label=False):\n",
    "    labels = pd.read_csv(os.path.join(prediction_dir, 'labels.csv'))\n",
    "\n",
    "    filepaths_train, filepaths_test = [], []\n",
    "    for filepath in sorted(glob.glob('{}/*'.format(prediction_dir))):\n",
    "        if filepath.endswith('predictions_train_oof.csv'):\n",
    "            filepaths_train.append(filepath)\n",
    "        elif filepath.endswith('predictions_test_oof.csv'):\n",
    "            filepaths_test.append(filepath)\n",
    "\n",
    "    train_dfs = []\n",
    "    for filepath in filepaths_train:\n",
    "        train_dfs.append(pd.read_csv(filepath))\n",
    "    train_dfs = reduce(lambda df1, df2: pd.merge(df1, df2, on=['id', 'fold_id']), train_dfs)\n",
    "    train_dfs.columns = _clean_columns(train_dfs, keep_colnames = ['id','fold_id'])\n",
    "    train_dfs = pd.merge(train_dfs, labels, on=['id'])\n",
    "\n",
    "    test_dfs = []\n",
    "    for filepath in filepaths_test:\n",
    "        test_dfs.append(pd.read_csv(filepath))\n",
    "    test_dfs = reduce(lambda df1, df2: pd.merge(df1, df2, on=['id', 'fold_id']), test_dfs)\n",
    "    test_dfs.columns = _clean_columns(test_dfs, keep_colnames = ['id','fold_id'])\n",
    "\n",
    "    return train_dfs, test_dfs\n",
    "\n",
    "def _clean_columns(df, keep_colnames):\n",
    "    new_colnames = []\n",
    "    for i,colname in enumerate(df.columns):\n",
    "        if colname not in keep_colnames:\n",
    "            new_colnames.append(i)\n",
    "        else:\n",
    "            new_colnames.append(colname)\n",
    "    return new_colnames\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "\n",
    "def get_fold_xy(train, test, label_columns,i):\n",
    "    train_split = train[train['fold_id'] != i]\n",
    "    valid_split = train[train['fold_id'] == i]\n",
    "    test_split = test[test['fold_id'] == i]\n",
    "\n",
    "    y_train = train_split[label_columns].values\n",
    "    y_valid = valid_split[label_columns].values\n",
    "    columns_to_drop_train = label_columns + ['id','fold_id']\n",
    "    X_train = train_split.drop(columns_to_drop_train, axis=1).values\n",
    "    X_valid = valid_split.drop(columns_to_drop_train, axis=1).values\n",
    "\n",
    "    columns_to_drop_test = ['id','fold_id']\n",
    "    X_test = test_split.drop(columns_to_drop_test, axis=1).values\n",
    "    return (X_train, y_train), (X_valid, y_valid), X_test\n",
    "    \n",
    "\n",
    "def stack_per_label(X, label_nr=6):\n",
    "    n,k =X.shape\n",
    "    model_nr = int(k/label_nr)\n",
    "    X_ = []\n",
    "    for i in range(model_nr):\n",
    "        X_.append(X[:,i*label_nr:(i+1)*label_nr])\n",
    "    X_ = np.stack(X_, axis=-1)\n",
    "    return X_\n",
    "\n",
    "def get_model_names(prediction_dir):\n",
    "    suffix = '_predictions_test_oof.csv'\n",
    "    filepaths = sorted([path.replace(suffix,'') for path in os.listdir(prediction_dir) if suffix in path])\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "SINGLE_DIR = '/public/toxic_comments/single_model_predictions_03092018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = read_predictions(SINGLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = get_model_names(SINGLE_DIR)\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(i, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's explore the label distributions and model correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "(X_train, y_train), (X_valid, y_valid), X_test = get_fold_xy(train, test, LABEL_COLUMNS,0)\n",
    "X_train_per_label = stack_per_label(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(LABEL_COLUMNS):\n",
    "    target_values = y_train[:,i]\n",
    "    mean_target = np.mean(target_values)\n",
    "    print(label, mean_target, int(1/mean_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, label in enumerate(LABEL_COLUMNS):\n",
    "    label_predictions = pd.DataFrame(X_train_per_label[:,i,:],columns = model_names)\n",
    "    correlation_mat = label_predictions.corr()\n",
    "    plt.title(label)\n",
    "    sns.heatmap(correlation_mat, vmin=0.0, vmax=1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cv(estimator, params, train, test, label_id, n_splits=10):\n",
    "    estimators,scores, test_predictions = [],[],[]   \n",
    "    for i in range(n_splits):\n",
    "        (X_train, y_train), (X_valid, y_valid), X_test = get_fold_xy(train, test, LABEL_COLUMNS,i)\n",
    "\n",
    "        y_train = y_train[:,label_id]\n",
    "        y_valid = y_valid[:,label_id]\n",
    "        \n",
    "        estimator_ = estimator(**params) \n",
    "        estimator_.fit(X_train, y_train,\n",
    "                       early_stopping_rounds=10,\n",
    "                       eval_metric=['error','auc'],\n",
    "                       eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                      verbose=False)        \n",
    "        y_valid_pred = estimator_.predict_proba(X_valid, ntree_limit=estimator_.best_ntree_limit)[:,1]\n",
    "        y_test_pred = estimator_.predict_proba(X_test, ntree_limit=estimator_.best_ntree_limit)[:,1]\n",
    "        score = roc_auc_score(y_valid, y_valid_pred)\n",
    "        estimators.append(estimator_)\n",
    "        scores.append(score)\n",
    "        test_predictions.append(y_test_pred)\n",
    "    return scores, estimators, test_predictions   \n",
    "\n",
    "def make_grid(param_grid):\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_dicts = [dict(zip(keys, v)) for v in product(*values)]\n",
    "    return param_dicts\n",
    "\n",
    "def plot_xgboost_learning_curve(model):\n",
    "    results = model.evals_result()\n",
    "    epochs = len(results['validation_0']['error'])\n",
    "    x_axis = range(0, epochs)\n",
    "\n",
    "    fig, [ax1,ax2] = plt.subplots(1,2, figsize=(14,6))\n",
    "    ax1.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "    ax1.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('XGBoost Log Loss')\n",
    "    \n",
    "    ax2.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "    ax2.plot(x_axis, results['validation_1']['auc'], label='Test')\n",
    "    ax2.legend()\n",
    "    ax2.set_title('XGBoost AUC')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores, predictions_test = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic\n",
    "\n",
    "The default model\n",
    "\n",
    "```python\n",
    "estimator = XGBClassifier\n",
    "\n",
    "params = dict(objective= 'rank:pairwise',\n",
    "                  eval_metric= 'auc',\n",
    "                  scale_pos_weight = 10,\n",
    "                  n_estimators= 500,\n",
    "                  learning_rate= 0.1,\n",
    "                  max_depth= 4,\n",
    "                  min_child_weight= 7,\n",
    "                  gamma=0.01,\n",
    "                  subsample= 1.0,\n",
    "                  colsample_bytree= 0.8, \n",
    "                  reg_lambda= 0.1, #1.0\n",
    "                  reg_alpha= 0.0,\n",
    "                  n_jobs=12\n",
    "                 )\n",
    "\n",
    "```\n",
    "\n",
    "gets **0.98704**  10-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "estimator = XGBClassifier\n",
    "      \n",
    "param_grid = dict(objective= ['rank:pairwise'],\n",
    "                  eval_metric= ['auc'],\n",
    "                  scale_pos_weight = [100],\n",
    "                  n_estimators= [500],\n",
    "                  learning_rate= [0.1],\n",
    "                  max_depth= [2,3,4,5],\n",
    "                  min_child_weight= [1,3,5,7],\n",
    "                  gamma=[0.01,0.05,0.1,0.5],\n",
    "                  subsample= [1.0,0.8,0.6],\n",
    "                  colsample_bytree= [0.4,0.6,0.8,1.0], \n",
    "                  reg_lambda= [0.0,0.01,0.1,0.5,1.0], #1.0\n",
    "                  reg_alpha= [0.0],\n",
    "                  n_jobs=[12]\n",
    "                 )\n",
    "\n",
    "label_id = 0\n",
    "nr_runs = 50\n",
    "grid_sample = np.random.choice(make_grid(param_grid), nr_runs, replace=False)\n",
    "\n",
    "grid_scores = []\n",
    "for params in tqdm(grid_sample):\n",
    "    scores, estimators, test_prediction = fit_cv(estimator, params, train, test, label_id, n_splits=10)  \n",
    "#     plot_xgboost_learning_curve(estimators[-1])\n",
    "#     print(params)\n",
    "    print('mean {} std {}'.format(np.mean(scores), np.std(scores)))\n",
    "    grid_scores.append((params, np.mean(scores)))\n",
    "    \n",
    "joblib.dump(grid_scores,'/mnt/ml-team/minerva/debug/toxic_grid.pkl')\n",
    "best_params = sorted(grid_scores, key= lambda x: x[1])[-1]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "estimator = XGBClassifier\n",
    "\n",
    "label_id = 0\n",
    "\n",
    "scores, estimators, test_prediction = fit_cv(estimator, best_params[0], train, test, label_id, n_splits=10)  \n",
    "valid_scores['toxic'] = scores\n",
    "predictions_test['toxic'] = test_prediction\n",
    "plot_xgboost_learning_curve(estimators[-1])\n",
    "print(params)\n",
    "print('mean {} std {}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severe Toxic\n",
    "\n",
    "The default model\n",
    "\n",
    "```python\n",
    "estimator = XGBClassifier\n",
    "    \n",
    "params = dict(objective= 'binary:logistic',\n",
    "  eval_metric= 'auc',\n",
    "  n_estimators= 125,\n",
    "  learning_rate= 0.1,\n",
    "  max_depth= 3,\n",
    "  min_child_weight= 10,\n",
    "  gamma=0.0,\n",
    "  subsample= 0.8,\n",
    "  colsample_bytree= 0.45, #0.3-0.5\n",
    "  reg_lambda= 0.2, #0.05\n",
    "  reg_alpha= 0.0,\n",
    "  n_jobs=12)\n",
    "\n",
    "```\n",
    "\n",
    "gets **0.99113**  10-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = XGBClassifier\n",
    "    \n",
    "param_grid = dict(objective= ['rank:pairwise'],\n",
    "                  eval_metric= ['auc'],\n",
    "                  scale_pos_weight = [100],\n",
    "                  n_estimators= [500],\n",
    "                  learning_rate= [0.1],\n",
    "                  max_depth= [2,3,4,5],\n",
    "                  min_child_weight= [1,3,5,7],\n",
    "                  gamma=[0.01,0.05,0.1,0.5],\n",
    "                  subsample= [1.0,0.8,0.6],\n",
    "                  colsample_bytree= [0.4,0.6,0.8,1.0], \n",
    "                  reg_lambda= [0.0,0.01,0.1,0.5,1.0], #1.0\n",
    "                  reg_alpha= [0.0],\n",
    "                  n_jobs=[12]\n",
    "                 )\n",
    "\n",
    "label_id = 1\n",
    "\n",
    "nr_runs = 50\n",
    "grid_sample = np.random.choice(make_grid(param_grid), nr_runs, replace=False)\n",
    "\n",
    "grid_scores = []\n",
    "for params in tqdm(grid_sample):\n",
    "    scores, estimators, test_prediction = fit_cv(estimator, params, train, test, label_id, n_splits=10)  \n",
    "#     plot_xgboost_learning_curve(estimators[-1])\n",
    "#     print(params)\n",
    "    print('mean {} std {}'.format(np.mean(scores), np.std(scores)))\n",
    "    grid_scores.append((params, np.mean(scores)))\n",
    "    \n",
    "joblib.dump(grid_scores,'/mnt/ml-team/minerva/debug/severe_toxic_grid.pkl')\n",
    "\n",
    "best_params = sorted(grid_scores, key= lambda x: x[1])[-1]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 1\n",
    "\n",
    "scores, estimators, test_prediction = fit_cv(estimator, best_params[0], train, test, label_id, n_splits=10)  \n",
    "valid_scores['severe_toxic'] = scores\n",
    "predictions_test['severe_toxic'] = test_prediction\n",
    "plot_xgboost_learning_curve(estimators[-1])\n",
    "print(params)\n",
    "print('mean {} std {}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obscene\n",
    "\n",
    "The default model\n",
    "\n",
    "```python\n",
    "estimator = XGBClassifier\n",
    "    \n",
    "params = dict(objective= 'binary:logistic',\n",
    "  eval_metric= 'auc',\n",
    "  n_estimators= 125,\n",
    "  learning_rate= 0.1,\n",
    "  max_depth= 3,\n",
    "  min_child_weight= 10,\n",
    "  gamma=0.0,\n",
    "  subsample= 0.8,\n",
    "  colsample_bytree= 0.45, #0.3-0.5\n",
    "  reg_lambda= 0.2, #0.05\n",
    "  reg_alpha= 0.0,\n",
    "  n_jobs=12)\n",
    "\n",
    "```\n",
    "\n",
    "gets **0.99501**  10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = XGBClassifier\n",
    "    \n",
    "param_grid = dict(objective= ['rank:pairwise'],\n",
    "                  eval_metric= ['auc'],\n",
    "                  scale_pos_weight = [18],\n",
    "                  n_estimators= [500],\n",
    "                  learning_rate= [0.1],\n",
    "                  max_depth= [2,3,4,5],\n",
    "                  min_child_weight= [1,3,5,7],\n",
    "                  gamma=[0.01,0.05,0.1,0.5],\n",
    "                  subsample= [1.0,0.8,0.6],\n",
    "                  colsample_bytree= [0.4,0.6,0.8,1.0], \n",
    "                  reg_lambda= [0.0,0.01,0.1,0.5,1.0], #1.0\n",
    "                  reg_alpha= [0.0],\n",
    "                  n_jobs=[12]\n",
    "                 )\n",
    "\n",
    "label_id = 2\n",
    "\n",
    "nr_runs = 50\n",
    "grid_sample = np.random.choice(make_grid(param_grid), nr_runs, replace=False)\n",
    "\n",
    "grid_scores = []\n",
    "for params in tqdm(grid_sample):\n",
    "    scores, estimators, test_prediction = fit_cv(estimator, params, train, test, label_id, n_splits=10)  \n",
    "#     plot_xgboost_learning_curve(estimators[-1])\n",
    "#     print(params)\n",
    "    print('mean {} std {}'.format(np.mean(scores), np.std(scores)))\n",
    "    grid_scores.append((params, np.mean(scores)))\n",
    "    \n",
    "joblib.dump(grid_scores,'/mnt/ml-team/minerva/debug/obscene.pkl')\n",
    "\n",
    "best_params = sorted(grid_scores, key= lambda x: x[1])[-1]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 2\n",
    "\n",
    "scores, estimators, test_prediction = fit_cv(estimator, best_params[0], train, test, label_id, n_splits=10)  \n",
    "valid_scores['obscene'] = scores\n",
    "predictions_test['obscene'] = test_prediction\n",
    "plot_xgboost_learning_curve(estimators[-1])\n",
    "print(params)\n",
    "print('mean {} std {}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threat\n",
    "\n",
    "The default model\n",
    "\n",
    "```python\n",
    "estimator = XGBClassifier\n",
    "    \n",
    "params = dict(objective= 'binary:logistic',\n",
    "  eval_metric= 'auc',\n",
    "  n_estimators= 125,\n",
    "  learning_rate= 0.1,\n",
    "  max_depth= 3,\n",
    "  min_child_weight= 10,\n",
    "  gamma=0.0,\n",
    "  subsample= 0.8,\n",
    "  colsample_bytree= 0.45, #0.3-0.5\n",
    "  reg_lambda= 0.2, #0.05\n",
    "  reg_alpha= 0.0,\n",
    "  n_jobs=12)\n",
    "\n",
    "```\n",
    "\n",
    "gets **0.99296**  10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = XGBClassifier\n",
    "    \n",
    "param_grid = dict(objective= ['rank:pairwise'],\n",
    "                  eval_metric= ['auc'],\n",
    "                  scale_pos_weight = [331],\n",
    "                  n_estimators= [500],\n",
    "                  learning_rate= [0.1],\n",
    "                  max_depth= [2,3,4,5],\n",
    "                  min_child_weight= [1,3,5,7],\n",
    "                  gamma=[0.01,0.05,0.1,0.5],\n",
    "                  subsample= [1.0,0.8,0.6],\n",
    "                  colsample_bytree= [0.4,0.6,0.8,1.0], \n",
    "                  reg_lambda= [0.0,0.01,0.1,0.5,1.0], #1.0\n",
    "                  reg_alpha= [0.0],\n",
    "                  n_jobs=[12]\n",
    "                 )\n",
    "\n",
    "label_id = 3\n",
    "\n",
    "nr_runs = 50\n",
    "grid_sample = np.random.choice(make_grid(param_grid), nr_runs, replace=False)\n",
    "\n",
    "grid_scores = []\n",
    "for params in tqdm(grid_sample):\n",
    "    scores, estimators, test_prediction = fit_cv(estimator, params, train, test, label_id, n_splits=10)  \n",
    "#     plot_xgboost_learning_curve(estimators[-1])\n",
    "#     print(params)\n",
    "    print('mean {} std {}'.format(np.mean(scores), np.std(scores)))\n",
    "    grid_scores.append((params, np.mean(scores)))\n",
    "    \n",
    "joblib.dump(grid_scores,'/mnt/ml-team/minerva/debug/threat_grid.pkl')\n",
    "\n",
    "best_params = sorted(grid_scores, key= lambda x: x[1])[-1]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 3\n",
    "\n",
    "scores, estimators, test_prediction = fit_cv(estimator, best_params[0], train, test, label_id, n_splits=10)  \n",
    "valid_scores['threat'] = scores\n",
    "predictions_test['threat'] = test_prediction\n",
    "plot_xgboost_learning_curve(estimators[-1])\n",
    "print(params)\n",
    "print('mean {} std {}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insult\n",
    "\n",
    "The default model\n",
    "\n",
    "```python\n",
    "estimator = XGBClassifier\n",
    "    \n",
    "params = dict(objective= 'binary:logistic',\n",
    "  eval_metric= 'auc',\n",
    "  n_estimators= 125,\n",
    "  learning_rate= 0.1,\n",
    "  max_depth= 3,\n",
    "  min_child_weight= 10,\n",
    "  gamma=0.0,\n",
    "  subsample= 0.8,\n",
    "  colsample_bytree= 0.45, #0.3-0.5\n",
    "  reg_lambda= 0.2, #0.05\n",
    "  reg_alpha= 0.0,\n",
    "  n_jobs=12)\n",
    "\n",
    "```\n",
    "\n",
    "gets **0.989208**  10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = XGBClassifier\n",
    "    \n",
    "param_grid = dict(objective= ['rank:pairwise'],\n",
    "                  eval_metric= ['auc'],\n",
    "                  scale_pos_weight = [20],\n",
    "                  n_estimators= [500],\n",
    "                  learning_rate= [0.1],\n",
    "                  max_depth= [2,3,4,5],\n",
    "                  min_child_weight= [1,3,5,7],\n",
    "                  gamma=[0.01,0.05,0.1,0.5],\n",
    "                  subsample= [1.0,0.8,0.6],\n",
    "                  colsample_bytree= [0.4,0.6,0.8,1.0], \n",
    "                  reg_lambda= [0.0,0.01,0.1,0.5,1.0], #1.0\n",
    "                  reg_alpha= [0.0],\n",
    "                  n_jobs=[12]\n",
    "                 )\n",
    "\n",
    "label_id = 4\n",
    "\n",
    "nr_runs = 50\n",
    "grid_sample = np.random.choice(make_grid(param_grid), nr_runs, replace=False)\n",
    "\n",
    "grid_scores = []\n",
    "for params in tqdm(grid_sample):\n",
    "    scores, estimators, test_prediction = fit_cv(estimator, params, train, test, label_id, n_splits=10)  \n",
    "#     plot_xgboost_learning_curve(estimators[-1])\n",
    "    print(params)\n",
    "    print('mean {} std {}'.format(np.mean(scores), np.std(scores)))\n",
    "    grid_scores.append((params, np.mean(scores)))\n",
    "    \n",
    "joblib.dump(grid_scores,'/mnt/ml-team/minerva/debug/insult_grid.pkl')\n",
    "\n",
    "best_params = sorted(grid_scores, key= lambda x: x[1])[-1]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 4\n",
    "\n",
    "scores, estimators, test_prediction = fit_cv(estimator, best_params[0], train, test, label_id, n_splits=10)  \n",
    "valid_scores['insult'] = scores\n",
    "predictions_test['insult'] = test_prediction\n",
    "plot_xgboost_learning_curve(estimators[-1])\n",
    "print(params)\n",
    "print('mean {} std {}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Hate\n",
    "\n",
    "The default model\n",
    "\n",
    "```python\n",
    "estimator = XGBClassifier\n",
    "    \n",
    "params = dict(objective= 'binary:logistic',\n",
    "  eval_metric= 'auc',\n",
    "  n_estimators= 125,\n",
    "  learning_rate= 0.1,\n",
    "  max_depth= 3,\n",
    "  min_child_weight= 10,\n",
    "  gamma=0.0,\n",
    "  subsample= 0.8,\n",
    "  colsample_bytree= 0.45, #0.3-0.5\n",
    "  reg_lambda= 0.2, #0.05\n",
    "  reg_alpha= 0.0,\n",
    "  n_jobs=12)\n",
    "\n",
    "```\n",
    "\n",
    "gets **0.990216**  10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator = XGBClassifier\n",
    "    \n",
    "param_grid = dict(objective= ['rank:pairwise'],\n",
    "                  eval_metric= ['auc'],\n",
    "                  scale_pos_weight = [112],\n",
    "                  n_estimators= [500],\n",
    "                  learning_rate= [0.1],\n",
    "                  max_depth= [2,3,4,5],\n",
    "                  min_child_weight= [1,3,5,7],\n",
    "                  gamma=[0.01,0.05,0.1,0.5],\n",
    "                  subsample= [1.0,0.8,0.6],\n",
    "                  colsample_bytree= [0.4,0.6,0.8,1.0], \n",
    "                  reg_lambda= [0.0,0.01,0.1,0.5,1.0], #1.0\n",
    "                  reg_alpha= [0.0],\n",
    "                  n_jobs=[12]\n",
    "                 )\n",
    "\n",
    "label_id = 5\n",
    "\n",
    "nr_runs = 50\n",
    "grid_sample = np.random.choice(make_grid(param_grid), nr_runs, replace=False)\n",
    "\n",
    "grid_scores = []\n",
    "for params in tqdm(grid_sample):\n",
    "    scores, estimators, test_prediction = fit_cv(estimator, params, train, test, label_id, n_splits=10)  \n",
    "#     plot_xgboost_learning_curve(estimators[-1])\n",
    "#     print(params)\n",
    "    print('mean {} std {}'.format(np.mean(scores), np.std(scores)))\n",
    "    grid_scores.append((params, np.mean(scores)))\n",
    "    \n",
    "joblib.dump(grid_scores,'/mnt/ml-team/minerva/debug/identity_hate_grid.pkl')\n",
    "\n",
    "best_params = sorted(grid_scores, key= lambda x: x[1])[-1]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = 5\n",
    "\n",
    "scores, estimators, test_prediction = fit_cv(estimator, best_params[0], train, test, label_id, n_splits=10)  \n",
    "valid_scores['identity_hate'] = scores\n",
    "predictions_test['identity_hate'] = test_prediction\n",
    "plot_xgboost_learning_curve(estimators[-1])\n",
    "print(params)\n",
    "print('mean {} std {}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cv_score = []\n",
    "for label, scores in valid_scores.items():\n",
    "    final_cv_score.append(np.mean(scores))\n",
    "    print(label, np.mean(scores),np.std(scores))\n",
    "print(np.mean(final_cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets explore out of fold predictions as explained here \n",
    "\n",
    "https://www.kaggle.com/ogrellier/things-you-need-to-be-aware-of-before-stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for l, label in enumerate(LABEL_COLUMNS):\n",
    "    predictions = predictions_test[label]\n",
    "    for fold_predictions in predictions:\n",
    "#         fold_predictions_logit = np.log((fold_predictions + 1e-5) / (1 - fold_predictions + 1e-5))\n",
    "#         sns.distplot(fold_predictions_logit, hist=False)\n",
    "        sns.distplot(fold_predictions, hist=False)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olivier's (kaggle kernel) ranking can be applied to out of fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_scaling(predictions):\n",
    "    predictions = pd.DataFrame(predictions)\n",
    "    predictions = (1 + predictions.rank().values) / (predictions.shape[0] + 1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, label in enumerate(LABEL_COLUMNS):\n",
    "    predictions = predictions_test[label]\n",
    "    for fold_predictions in predictions:\n",
    "        fold_predictions = oof_scaling(fold_predictions)\n",
    "#         fold_predictions_logit = np.log((fold_predictions + 1e-5) / (1 - fold_predictions + 1e-5))\n",
    "#         sns.distplot(predictions_logit, hist=False)\n",
    "        sns.distplot(fold_predictions, hist=False)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions = {}\n",
    "for label, predictions in predictions_test.items():\n",
    "    oof_predictions = [oof_scaling(fold_predictions) for fold_predictions in predictions]\n",
    "    oof_predictions_mean = np.mean(np.stack(oof_predictions, axis=-1),axis=-1).reshape(-1)\n",
    "    combined_predictions[label] = oof_predictions_mean.tolist()\n",
    "combined_predictions = pd.DataFrame(combined_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_SUBMISSION_PATH = '/output/catboost_submission.csv'\n",
    "\n",
    "submission = pd.read_csv('/public/toxic_comments/single_model_predictions_03092018/sample_submission.csv')\n",
    "submission[LABEL_COLUMNS] = combined_predictions[LABEL_COLUMNS].values \n",
    "submission.to_csv(ENSEMBLE_SUBMISSION_PATH, index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_py3",
   "language": "python",
   "name": "dl_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
