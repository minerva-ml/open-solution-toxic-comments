project-key: TOX

name: toxic
tags: [ensemble, xgboost, predictions]


metric:
  channel: 'Final Validation Score ROC_AUC'
  goal: maximize

# Comment out if not in Cloud Environment
pip-requirements-file: requirements.txt

exclude:
  - output
  - imgs
  - neptune.log
  - offline_job.log
  - .git
  - .idea
  - .ipynb_checkpoints

parameters:
# Cloud Environment
  data_dir:                     /public/toxic_comments
  embedding_filepath:           /public/models/word2vec/GoogleNews-vectors-negative300.bin
  single_model_predictions_dir: /public/toxic_comments/single_model_predictions_03092018
  experiment_dir:               /output/trained_pipelines/xgboost_ensemble

# Local Environment
#  data_dir:                     /path/to/toxic/data
#  embedding_filepath:           /path/to/embedding i.e. ~/word2vec/GoogleNews-vectors-negative300.bin
#  single_model_predictions_dir: /path/to/single/model/preds/ i.e. ~/single_model_predictions_03092018
#  experiment_dir:               /my/working/directory i.e. ~/toxic/trained_pipelines/xgboost_ensemble

  bad_words_filepath: None
  overwrite: 1
  num_workers: 4
  n_cv_splits: 10

# Preprocessing
  max_features_char: None # 100 for vdcnn, for tfidf something like 20000 should be used
  max_features_word: None
  maxlen_char: None
  maxlen_words: None
  char_ngram_max: None
  truncated_svd__n_components: None
  drop_punctuation: None # Do I want to preprocess in that way for all models?
  drop_newline: None
  drop_multispaces: None
  all_lower_case: None
  fill_na_with: None
  deduplication_threshold: None
  anonymize: 0
  apostrophes: 0
  use_stopwords: 0

# Architecture
  filter_nr: None
  kernel_size: None
  repeat_block: None
  dense_size: None
  repeat_dense: None
  max_pooling: None
  mean_pooling: None
  weighted_average_attention: None
  concat_mode:  None
  trainable_embedding: None
  word_embedding_size: None
  char_embedding_size: None

# General Architecture
  use_prelu: None

# Log Reg Params
  log_reg_c: None
  log_reg_penalty: None
  max_iter: None

# Training schedule
  epochs_nr: None
  batch_size_train: None
  batch_size_inference: None
  lr: None
  momentum: None
  gamma: None
  patience: None
  unfreeze_on_epoch: None

# Regularization
  batch_norm_first: None
  use_batch_norm: None
  dropout_embedding: None
  rnn_dropout: None
  dense_dropout: None
  conv_dropout: None
  dropout_mode: None
  rnn_kernel_reg_l2: None
  rnn_recurrent_reg_l2: None
  rnn_bias_reg_l2: None
  dense_kernel_reg_l2: None
  dense_bias_reg_l2: None
  conv_kernel_reg_l2: None
  conv_bias_reg_l2: None

# Ensemble Catboost
  catboost__iterations: None
  catboost__learning_rate: None
  catboost__depth: None
  catboost__l2_leaf_reg: None
  catboost__border_count: None
  catboost__model_size_reg: None
  catboost__rsm: None
  catboost__verbose: None

# Ensemble Blender
  blender__method: None
  blender__runs: None
  blender__maxiter: None

# Ensemble XGBoost
  xgboost__objective: 'binary:logistic'
  xgboost__eval_metric: 'auc'
  xgboost__scale_pos_weight: 10
  xgboost__n_estimators: 125
  xgboost__learning_rate: 0.1
  xgboost__max_depth: 3
  xgboost__min_child_weight: 10
  xgboost__gamma: 0.0
  xgboost__subsample: 0.8
  xgboost__colsample_bytree: 0.45
  xgboost__reg_lambda:  0.2
  xgboost__reg_alpha: 0.0

# Postprocessing
  clipper__lower: None
  clipper__upper: None
