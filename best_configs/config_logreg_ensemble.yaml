project-key: TOX

name: toxic
tags: [ensemble, logreg, predictions]


metric:
  channel: 'Final Validation Score'
  goal: minimize

#Comment out if not in Cloud Environment
#pip-requirements-file: requirements.txt

exclude:
  - output
  - imgs
  - neptune.log
  - offline_job.log
  - .git
  - .idea
  - .ipynb_checkpoints

parameters:
  # Cloud Environment
#  data_dir: /public/toxic_comments
#  experiment_dir: /output/trained_pipelines/glove_lstm
#  embedding_filepath: /public/models/glove/glove.840B.300d.txt
  # Local Environment
  data_dir: /mnt/ml-team/minerva/toxic/data
  experiment_dir: /mnt/ml-team/minerva/toxic/trained_pipelines/logreg_ensemble_best
  embedding_filepath: /mnt/ml-team/minerva/pretrained/glove.840B.300d.txt
  bad_words_filepath: external_data/compiled_bad_words.txt
  overwrite: 0
  num_workers: -2

  # Preprocessing
  max_features_char: 100 #100 for vdcnn, for tfidf something like 20000 should be used
  max_features_word: 100000
  maxlen_char: 512
  maxlen_words: 100
  char_ngram_max: 4
  drop_punctuation: 1
  drop_newline: 1
  drop_multispaces: 1
  all_lower_case: 1
  fill_na_with: ' '


  # Glove Deep Pyramid Architecture
#  filter_nr: 64
#  kernel_size: 3
#  repeat_block: 2
#  dense_size: 256
#  repeat_dense: 1
#  trainable_embedding: 1
#  word_embedding_size: 300
#  char_embedding_size: None
#  global_pooling: 1

  # Glove Shallow CNN Architecture
#  filter_nr: 256
#  kernel_size: 6
#  repeat_block: None
#  dense_size: 256
#  repeat_dense: 1
#  trainable_embedding: 1
#  word_embedding_size: 300
#  char_embedding_size: None
#  global_pooling: 1

  # Char VDCNN Architecture
#  filter_nr: 128
#     value: [64, 128]
#  kernel_size: 3
#  repeat_block: 3
#     value: [2, 3, 4, 5]
#  global_pooling: 1
#  trainable_embedding: 1
#  dense_size: 256
#     value: [256, 512, 1024]
#  repeat_dense: 1
#     value: [1, 2]
#  char_embedding_size: 16
#  word_embedding_size: None


  # Glove LSTM Architecture
#  filter_nr: 64
#  kernel_size: None
#  repeat_block: 2
#  dense_size: 256
#  repeat_dense: 2
#  global_pooling: 1
#  trainable_embedding: 0
#  word_embedding_size: 300
#  char_embedding_size: None

  # Word LSTM Architecture
  filter_nr: 128
  kernel_size: None
  repeat_block: 3
  dense_size: 256
  repeat_dense: 2
  global_pooling: 1
  trainable_embedding: 1
  word_embedding_size: 300
  char_embedding_size: None

  # General Architecture
  use_prelu: 0

 # Log Reg Params
  log_reg_c: 4.0
  log_reg_penalty: 'l2'
  max_iter: 1000

  # SVM Params
  svm_C: 0.001

  # Ensemble weighted average
  weights: None

  # Ensemble Log Reg
  ensemble_log_reg_c: 1.0

  # Ensemble Random Forest
  rf_n_estimators: 1000

  # Training schedule
  epochs_nr: 1000
  batch_size_train: 32
  batch_size_inference: 128
  lr: 0.01
  momentum: 0.9
  gamma: 0.97
  patience: 10

  # Regularization
  use_batch_norm: 0
  l2_reg_convo: 0.0
  l2_reg_dense: 0.0
  dropout_lstm: 0.2
  dropout_convo: 0.2
  dropout_dense: 0.0